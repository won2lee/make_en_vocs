{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "durable-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from util_func import json_read, json_save, bpe_vocab_iteration\r\n",
      "\r\n",
      "path = 'data/bpe_iter/'\r\n",
      "\r\n",
      "step_from_start = 7000\r\n",
      "iter_size = 10000\r\n",
      "step_size = 250\r\n",
      "\r\n",
      "def main():\r\n",
      "\r\n",
      "    if step_from_start == 0:\r\n",
      "        vocabs = json_read('data/en_vocabs/vocabs.json')\r\n",
      "        vocabs = {' '.join([c for c in k]):v for k,v in vocabs.items()}\r\n",
      "        vocabs_freq = {}\r\n",
      "        json_save(vocabs,path+'vocabs'+str(0))\r\n",
      "        json_save(vocabs_freq,path+'vocs_freq'+str(0))\r\n",
      "\r\n",
      "    new_bpe_vocabs, vocabs_freq =bpe_vocab_iteration(path, iter_size,step_from_start, step_size)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!cat bpe_iter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "direct-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]\n",
      "[['out', 'lin'], ['br', 'un'], ['port', 'land'], ['in', 'cheon'], ['mar', 'vel'], ['stan', 'ley'], ['em', 'power'], ['ther', 'ine'], ['bre', 'ast'], ['bio', 'graphy'], ['spec', 'trum'], ['er', 'up'], ['no', 'd'], ['nin', 'th'], ['frame', 'work'], ['193', '1'], ['acc', 'um'], ['ker', 'ry'], ['ra', 'ham'], ['ag', 'ram'], ['pent', 'agon'], ['6', '1'], ['excess', 'ive'], ['l', 'ate'], ['sen', 'se'], ['pre', 'fect'], ['casual', 'ties'], ['-', '19'], ['re', 'fin'], ['n', 'an'], ['s', 'ri'], ['ste', 'ep'], ['prim', 'aries'], ['nel', 'son'], ['dy', 'ing'], ['lux', 'ury'], ['t', 'issue'], ['collect', 'ion'], ['in', 'er'], ['av', 'e'], ['rel', 'uct'], ['to', 'mb'], ['touch', 'down'], ['le', 'ban'], ['n', 'ant'], ['young', '-'], ['in', 'cent'], ['ou', 'trag'], ['land', 'mark'], ['nix', 'on']]\n",
      "[['p', 'am'], ['bo', 'd'], ['copy', 'right'], ['scrut', 'iny'], ['in', 'car'], ['19', '23'], ['ef', 'ficiency'], ['k', 'um'], ['robin', 'son'], ['heav', 'en'], ['commem', 'orat'], ['pho', 'en'], ['hope', 'ful'], ['-', 'off'], ['hold', 'ing'], ['g', 'em'], ['any', 'body'], ['ach', 'ieve'], ['ver', 'g'], ['s', 'man'], ['kinder', 'garten'], ['max', 'im'], ['after', 'math'], ['judic', 'iary'], ['ind', 'ic'], ['dis', 'patch'], ['bi', 'ological'], ['korea-', 'u.s'], ['-', 'to-'], ['a', 'hu'], ['consol', 'idat'], ['pet', 'ro'], ['ol', 'ition'], ['ho', 'o'], ['sh', 'ah'], ['ar', 'sen'], ['anony', 'mous'], ['cot', 'ton'], ['19', '27'], ['a', 'then'], ['har', 'sh'], ['unemploy', 'ment'], ['al', 'gor'], ['direct', 'ion'], ['fil', 'ter'], ['ar', 'is'], ['19', '15'], ['og', 'ue'], ['inter', 'n'], ['hy', 'und']]\n",
      "[['fer', 't'], ['cu', 'le'], ['c', 'ant'], ['hyund', 'ai'], ['provis', 'ion'], ['cy', 'lin'], ['le', 'vi'], ['caval', 'ry'], ['die', 'sel'], ['algor', 'ith'], ['de', 'tent'], ['bl', 'ad'], ['path', 'y'], ['but', 'ton'], ['fac', 'ilitat'], ['over', 'see'], ['jim', 'my'], ['aar', 'on'], ['pne', 'um'], ['pregn', 'ancy'], ['st', 'uck'], ['bi', 'k'], ['mu', 'hammad'], ['su', 'san'], ['hear', 'ing'], ['g', 'ros'], ['sh', 'ad'], ['cult', 'ivat'], ['cl', 'u'], ['trag', 'edy'], ['g', 'ut'], ['wh', 'il'], ['jae', '-'], ['po', 'sed'], ['sh', 'u'], ['so-', 'called'], ['bound', 'aries'], ['car', 'ib'], ['some', 'how'], ['freder', 'ick'], ['recogn', 'is'], ['la', 'de'], ['17', '0'], ['tr', 'ap'], ['m', 'our'], ['psych', 'ological'], ['con', 'trad'], ['mak', 'ing'], ['ri', 'b'], ['o', 'val']]\n",
      "[['pre', 'dominant'], ['can', \"'t\"], ['mon', 'k'], ['aw', 'l'], ['cour', 'se'], ['whil', 'st'], ['pre', 'cis'], ['7', '4'], ['o', 'ir'], ['pre', 'lim'], ['hol', 'land'], ['start', 'up'], ['cap', 'abilities'], ['dan', 'ish'], ['eng', 'age'], ['collab', 'or'], ['propag', 'anda'], ['log', 'ic'], ['rad', 'iat'], ['vice', '-'], ['dem', 'olish'], ['be', 'fore'], ['li', 'de'], ['so', 'o'], ['j', 'enn'], ['ep', 'ide'], ['ast', 'ery'], ['19', '12'], ['bu', 'yer'], ['pay', 'er'], ['10', '.'], ['in', 'mate'], ['explo', 'd'], ['al', 'locat'], ['inspect', 'or'], ['sor', 'ry'], ['half', '-'], ['is', '.'], ['cl', 'uster'], ['as', 'pir'], ['h', 'iv'], ['un', 've'], ['a', 'j'], ['ist', 'a'], ['tim', 'ber'], ['sal', 'ary'], ['dev', 'il'], ['fle', 'w'], ['inquir', 'y'], ['auto', 'mat']]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"bpe_iter.py\", line 22, in <module>\n",
      "    main()\n",
      "  File \"bpe_iter.py\", line 19, in main\n",
      "    new_bpe_vocabs, vocabs_freq =bpe_vocab_iteration(path, iter_size,step_from_start, step_size)\n",
      "  File \"/home/john/Notebook/preProject/create_vocabs_en/util_func.py\", line 62, in bpe_vocab_iteration\n",
      "    new_bpe_vocabs,vocabs_f,_,_ = vocab_select2(vocabs, vocabs_freq, path, iter_size, step_size, step_from_start, save_cyc)\n",
      "  File \"/home/john/Notebook/preProject/create_vocabs_en/util_func.py\", line 71, in vocab_select2\n",
      "    vocabs, vocabs_freq = dpe_iteration2(vocabs, vocabs_freq, step_size)\n",
      "  File \"/home/john/Notebook/preProject/create_vocabs_en/util_func.py\", line 117, in dpe_iteration2\n",
      "    pairs = get_stats(vocab)\n",
      "  File \"/home/john/Notebook/preProject/create_vocabs_en/util_func.py\", line 92, in get_stats\n",
      "    for i in range(len(symbols)-1):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 bpe_iter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import json_read, json_save, bpe_vocab_iteration\n",
    "\n",
    "path = 'data/bpe_iter/'\n",
    "\n",
    "step_from_start = 0\n",
    "iter_size = 10000\n",
    "step_size = 250\n",
    "\n",
    "def main():\n",
    "\n",
    "    if step_from_start == 0:\n",
    "        vocabs = json_read('data/en_vocabs/vocabs.json')\n",
    "        vocabs = {' '.join([c for c in k]):v for k,v in vocabs.items()}\n",
    "        vocabs_freq = {}\n",
    "        json_save(vocabs,path+'vocabs'+str(0))\n",
    "        json_save(vocabs_freq,path+'vocs_freq'+str(0))\n",
    "\n",
    "    new_bpe_vocabs, vocabs_freq =bpe_vocab_iteration(path, iter_size,step_from_start, step_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
